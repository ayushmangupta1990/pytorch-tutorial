{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--train_epoch'], dest='train_epoch', nargs=None, const=None, default=32, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Mnist Arg Parser')\n",
    "parser.add_argument('--train_epoch',type=int,default=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pin_memory: speed up the data transfer with pinned RAM (Only helpful for GPU)\n",
    "\n",
    "# num_worker: asyncronous use of data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root='data',train=True,download=False, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))\n",
    "]))\n",
    "test_data  = datasets.MNIST(root='data',train=False,download=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_data_loader = DataLoader(train_data,shuffle=True,batch_size=64,num_workers=2, pin_memory=True)\n",
    "test_data_loader  = DataLoader(test_data,shuffle=False,batch_size=64,num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculation of flatern layer\n",
    "# ((image height - kernel size+1 )/2   * (image width - kernel size +1) /2     )* number of filters\n",
    "# \n",
    "# \n",
    "#  nllloss  -- log_softmax\n",
    "#\n",
    "#################\n",
    "#One way to cut the computation graph is to use .detach(), which you may use when passing on a hidden \n",
    "#state when training RNNs with truncated backpropagation-through-time. It's also handy when differentiating\n",
    "#a loss where one component is the output of another network, but this other network shouldn't be optimised \n",
    "#with respect to the loss - examples include training a discriminator from a generator's outputs in GAN training,\n",
    "#or training the policy of an actor-critic algorithm using the value function as a baseline (e.g. A2C). \n",
    "#Another technique for preventing gradient calculations that is efficient in GAN training (training the generator \n",
    "#from the discriminator) and typical in fine-tuning is to loop through a networks parameters and set param.requires_grad = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.Conv1   =  nn.Conv2d(1,10,kernel_size=5)\n",
    "        self.Conv2   =  nn.Conv2d(10,20,kernel_size=5)\n",
    "        self.conv_drop = nn.Dropout2d(.2)\n",
    "        self.fc1     =  nn.Linear(320,128)\n",
    "        self.fc2     =  nn.Linear(128,10)\n",
    "    def forward(self,x):\n",
    "        out =  F.relu(F.max_pool2d(self.Conv1(x),2))\n",
    "        out =  F.relu(F.max_pool2d(self.conv_drop(self.Conv2(out)),2))\n",
    "        #print('out1 shape ',out.size())\n",
    "        out =  out.view(-1,320)\n",
    "        out =  self.fc1(out)\n",
    "        out =  self.fc2(out)\n",
    "        out =  F.log_softmax(out,dim=1)\n",
    "        #print('outLen ',len(out))\n",
    "        return out\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "\n",
    "optimizer = torch.optim.SGD(lr=0.001,params=model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss 1.3742467608681181\n",
      "Epoch Loss 1.395789196511032\n",
      "Epoch Loss 1.3499742877320386\n",
      "Epoch Loss 1.3723979643691564\n",
      "Epoch Loss 1.3350034433969995\n",
      "Epoch Loss 1.3301743593765423\n",
      "Epoch Loss 1.3098389453880372\n",
      "Epoch Loss 1.2878030736756045\n",
      "Epoch Loss 1.2910278917042888\n",
      "Epoch Loss 1.2581438823108329\n",
      "Epoch Loss 1.2549563892462174\n",
      "Epoch Loss 1.2428965955332387\n",
      "Epoch Loss 1.219821699996828\n",
      "Epoch Loss 1.2083377115268377\n",
      "Epoch Loss 1.2043094441105495\n",
      "Epoch Loss 1.1814545951965556\n",
      "Epoch Loss 1.1877205644486821\n",
      "Epoch Loss 1.1746998113012523\n",
      "Epoch Loss 1.1477801086075488\n",
      "Epoch Loss 1.15787238865596\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Training\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "    for i,(images,labels) in enumerate(train_data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        #print(len(output))\n",
    "        loss = F.nll_loss(output,labels)\n",
    "        torch.max(output)\n",
    "        epoch_loss.append(loss.item()/64)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Epoch Loss', sum(epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 97.940000\n"
     ]
    }
   ],
   "source": [
    "# Evaluating \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    for epoch in range(1):\n",
    "\n",
    "        for i,(images,labels) in enumerate(test_data_loader):\n",
    "            output = model(images)\n",
    "            pred = output.argmax(dim=1,keepdim=True)\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    print(\"Accuracy {:2f}\".format(correct/len(test_data_loader.dataset)*100))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
